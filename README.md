<h1 align="center">🔬 PEIT: Property Enhanced Instruction Tuning for Multi-task Molecular Generation with LLMs</h1>

<p align="center">
  <strong>Official GitHub Repository for the PEIT Framework</strong><br>
  Integrating <code>PEIT-GEN</code> and <code>PEIT-LLM</code> for unified molecule representation, prediction, and generation.
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2412.18084"><img src="https://img.shields.io/badge/arXiv-2412.18084-b31b1b.svg" alt="arXiv"></a>
  <a href="https://huggingface.co/ccsalong/PEIT-LLM-LLaMa3.1-8B/tree/main"><img src="https://img.shields.io/badge/HuggingFace-PEIT--LLM-yellow" alt="HF Checkpoints"></a>
  <a href="https://pan.baidu.com/s/1VcFvrVHmjBZpL2L_QWt9TQ?pwd=vvts"><img src="https://img.shields.io/badge/Instruction%20Data-Baidu%20Pan-blue" alt="Baidu Link"></a>
</p>

---

## 🧠 Overview

This repository introduces **PEIT**, a framework for **Property Enhanced Instruction Tuning** that bridges molecular **structure**, **property**, and **text** for **multi-task molecular generation**. It includes:

- **PEIT-GEN**: A multimodal generation model pre-trained to align molecular structures, descriptions, and properties.
- **PEIT-LLM**: A fine-tuned large language model for instruction-based molecular understanding and generation.

<p align="center">
  <img src="https://github.com/user-attachments/assets/e8a7d4bf-c624-42ca-8a54-9cc75681396d" width="90%" />
</p>

---

## 📦 What’s Included

```text
.
├── Template_Generation/        # Templates for downstream molecular tasks
├── PEIT_pretrain.py            # Pre-training script for PEIT-GEN
├── calc_property.py            # RDKit-based molecular property calculator
├── llama3_lora_sft.yaml        # SFT config for LLaMa3.1-based PEIT-LLM
├── example_*                   # Example completions generated by PEIT-GEN
```

---

## 🛠️ Installation

Clone this repository and install dependencies:

```bash
git clone https://github.com/your-repo/PEIT.git
cd PEIT
pip install -r requirements.txt
```

> 🧪 If RDKit installation fails via pip, try:
```bash
conda install -c rdkit rdkit
```

---

## 🚀 Usage

### 🔧 1. Pretrain PEIT-GEN

```bash
python PEIT_pretrain.py --data_path ./data/pretrain_data.csv
```

Optional arguments:
- `--epochs`: number of training epochs
- `--batch_size`: batch size
- `--lr`: learning rate

Example:

```bash
python PEIT_pretrain.py --data_path ./data/pretrain_data.csv --epochs 10 --batch_size 64
```

---

### 🧠 2. Fine-tune PEIT-LLM

```bash
llamafactory-cli train llama3_lora_sft.yaml
```

Make sure to update paths inside `llama3_lora_sft.yaml`:
- `model_name_or_path`
- `dataset_dir`
- `output_dir`

---

### 🧬 3. Molecular Property Calculation (Optional)

```bash
python calc_property.py --input ./data/input.csv --output ./data/output.csv
```

---

### 📜 4. Generate Instruction Templates

Run templates from `Template_Generation`:

```bash
python Template_Generation/generate_caption_template.py
```

You can customize these for:
- Captioning
- Property prediction
- Molecule generation
- Multi-constraint generation
