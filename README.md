<h1 align="center">   PEIT: a framework for Property Enhanced Instruction Tuning for multi-task molecular generation with LLMs.  </h1>
<h3 align="center">  </h3>
The official GitHub repository for PEIT includes a multimodal molecular information generation model, PEIT-GEN, designed for synergistic comprehension of molecular structures, properties, and descriptions. Additionally, it features a specialized large language model, PEIT-LLM, fine-tuned through filling-based multitask template instruction tuning. 

***<ins>Further details can be found in the following [arXiv paper](https://arxiv.org/abs/2412.18084)<ins>***

We have initially uploaded the PEIT-GEN pretraining code (PEIT_pretrain.py) along with the template generation code for various tasks in the Template_Generation module.
<p style="text-align: center;">
  <img src="https://github.com/user-attachments/assets/4224b90a-bdab-402b-bec7-96b64ac7b2f2" width="100%" height="100%" />
</p>

***<ins>The PEIT-LLMs model checkpoint and data are too heavy to be included in this repo, and they can be found [here](https://huggingface.co/ccsalong/PEIT-LLM-LLaMa3.1-8B/tree/main).<ins>***

***<ins>Additionally, we have made part of the LLM instruction dataset publicly available at [here](https://pan.baidu.com/s/1VcFvrVHmjBZpL2L_QWt9TQ?pwd=vvts).<ins>*** The remaining code will be fully released in the near future.
## Files
* `Template_Generation/`: Contains the all the template filling code of four downstream tasks.
* `PEIT_pretrain.py/` runs PEIT-GEN pre-training.
* `calc_property.py/` Codes for calculate the molecular properties.
* `llama3_lora_sft.yaml` runs PEIT-LLMs sft.
* `example_*` The examples generated by PEIT-GEN for SFT.
## Requirements
Run `pip install -r requirements.txt` to install the required packages.
## Code running
Arguments can be passed with commands, or be edited manually in the running code.
1. Pre-training for PEIT-GEN
    ```
    python PEIT_pretrain.py --data_path './data/pretrain_data.csv'
    ```
2. SFT for PEIT-LLM
    ```
    llamafactory-cli train llama3_lora_sft.yaml
    ```
## Experimental Results

#### Molecule Captioning
| Model | BLEU-2 | BLEU-4 | METEOR | ROUGE-1 | ROUGE-2 | ROUGE-L |
|-------|--------|--------|--------|---------|---------|---------|
| PEIT-GEN | **0.598** | **0.534** | **0.676** | **0.700** | **0.582** | **0.653** |
| PEIT-LLM-LLaMa3.1 | **0.461** | **0.356** | **0.502** | **0.569** | **0.396** | **0.505** |
| PEIT-LLM-Qwen2.5 | **0.422** | **0.314** | **0.468** | **0.535** | **0.361** | **0.477** |

#### Molecular Property Prediction
| Model | RMSE | R2 |
|-------|----|------|
| PEIT-GEN | **0.169** | **0.910** |

#### Molecular Property Prediction on MoleculeNet 
| Model | BBBP | BACE | Clintox | SIDER |
|-------|------|------|---------|-------|
| PEIT-GEN | **73.6±0.7** | **81.6±0.5** | **91.2±0.7** | **62.7±0.9** |

#### Text-Based Molecule Generation
| Model | BLEU | Validity | Levenshtein | MACCS FTS | Morgan FTS | RDKit FTS |
|-------|------|---------|------------|----------|-----------|----------|
| PEIT-LLM-LLaMa3.1 | **0.836** | **0.970** | **18.030** | **0.875** | **0.661** | **0.776** |
| PEIT-LLM-Qwen2.5 | **0.810** | **0.950** | **21.133** | **0.832** | **0.619** | **0.735** |

#### Multi-Constraint Molecule Generation
| Model | RMSE | R2 |
|-------|------|----|
| PEIT-LLM-LLaMa3.1 | **14.212** | **0.613** |
| PEIT-LLM-Qwen2.5 | **19.750** | **0.550** |

### Citation
If you found our work useful, please cite:
```bibtex
@article{lin2024property,
  title={Property Enhanced Instruction Tuning for Multi-task Molecule Generation with Large Language Models},
  author={Lin, Xuan and Chen, Long and Wang, Yile and Zeng, Xiangxiang and Yu, Philip S},
  journal={arXiv preprint arXiv:2412.18084},
  year={2024}
}
```
